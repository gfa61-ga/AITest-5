[
  {
    "question": "Ποιο είναι το βασικό χαρακτηριστικό της Ενισχυτικής Μάθησης (RL);",
    "answer": "A",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 234",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Η μάθηση μέσω αλληλεπίδρασης με το περιβάλλον και η χρήση ανατροφοδότησης (ανταμοιβών/ποινών).",
      "B": "Η μάθηση αποκλειστικά από στατικά δεδομένα χωρίς αλληλεπίδραση.",
      "C": "Η εκπαίδευση χωρίς καμία ανταμοιβή ή ποινή.",
      "D": "Η χρήση μόνο επιβλεπόμενων μεθόδων."
    },
    "explanation": "Η ενισχυτική μάθηση διαφέρει από άλλες μεθόδους καθώς βασίζεται στη δοκιμή και το λάθος, όπου ο πράκτορας προσαρμόζεται βάσει του αποτελέσματος των ενεργειών του."
  },
  {
    "question": "Ποια είναι η διαφορά μεταξύ της 'Συνάρτησης Τιμής' (Value Function) και της 'Πολιτικής' (Policy);",
    "answer": "B",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 236",
      "paragraph": "2η & 3η παράγραφος"
    },
    "choices": {
      "A": "Η Συνάρτηση Τιμής καθορίζει την ενέργεια, ενώ η Πολιτική την αξία της κατάστασης.",
      "B": "Η Πολιτική καθορίζει τη συμπεριφορά (ποια ενέργεια να επιλεγεί), ενώ η Συνάρτηση Τιμής εκτιμά το αναμενόμενο μελλοντικό όφελος μιας κατάστασης.",
      "C": "Είναι ταυτόσημες έννοιες και χρησιμοποιούνται εναλλακτικά.",
      "D": "Η Πολιτική χρησιμοποιείται μόνο σε στατικά περιβάλλοντα."
    },
    "explanation": "Η πολιτική είναι ο 'χάρτης' δράσης του πράκτορα, ενώ η συνάρτηση τιμής είναι η 'πρόβλεψη' του πόσο καλή είναι η τρέχουσα θέση του στο παιχνίδι."
  },
  {
    "question": "Τι είναι το 'Deep Reinforcement Learning' (DRL);",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 237",
      "paragraph": "2η παράγραφος"
    },
    "choices": {
      "A": "Ένας τύπος μάθησης χωρίς νευρωνικά δίκτυα.",
      "B": "Μια μέθοδος για απλά προβλήματα με λίγες καταστάσεις.",
      "C": "Ο συνδυασμός ενισχυτικής μάθησης με βαθιά νευρωνικά δίκτυα για τη διαχείριση πολύπλοκων περιβαλλόντων υψηλών διαστάσεων.",
      "D": "Μια τεχνική που χρησιμοποιείται μόνο για επεξεργασία κειμένου."
    },
    "explanation": "Το DRL επιτρέπει στους πράκτορες να κατανοούν σύνθετα δεδομένα (π.χ. εικόνα από βιντεοπαιχνίδι) και να μαθαίνουν στρατηγικές που ήταν αδύνατες με την απλή RL."
  },
  {
    "question": "Ποιο είναι το πλεονέκτημα των μεθόδων 'Actor-Critic';",
    "answer": "D",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 238",
      "paragraph": "2η παράγραφος"
    },
    "choices": {
      "A": "Χρησιμοποιούν μόνο έναν πράκτορα.",
      "B": "Είναι πιο αργές από τις απλές μεθόδους.",
      "C": "Βασίζονται αποκλειστικά σε τυχαίες κινήσεις.",
      "D": "Συνδυάζουν τα οφέλη των μεθόδων αξίας (Critic) και πολιτικής (Actor) για πιο σταθερή και γρήγορη μάθηση."
    },
    "explanation": "Στο Actor-Critic, ο 'Ηθοποιός' προτείνει ενέργειες και ο 'Κριτικός' αξιολογεί πόσο καλές ήταν, επιτρέποντας πιο στοχευμένη βελτίωση."
  },
  {
    "question": "Τι χαρακτηρίζει τα 'Πολυπρακτορικά Συστήματα' (Multi-Agent Systems);",
    "answer": "A",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 239",
      "paragraph": "2η παράγραφος"
    },
    "choices": {
      "A": "Η ταυτόχρονη παρουσία και αλληλεπίδραση πολλών πρακτόρων που μπορεί να συνεργάζονται ή να ανταγωνίζονται.",
      "B": "Η χρήση ενός μόνο πανίσχυρου πράκτορα.",
      "C": "Η απουσία αλληλεπίδρασης μεταξύ των πρακτόρων.",
      "D": "Η λειτουργία σε περιβάλλοντα χωρίς αβεβαιότητα."
    },
    "explanation": "Τα MAS μοντελοποιούν πολύπλοκα κοινωνικά ή οικονομικά φαινόμενα όπου η απόφαση του ενός επηρεάζει το αποτέλεσμα των άλλων (π.χ. κυκλοφορία, χρηματιστήριο)."
  },
  {
    "question": "Τι είναι το δίλημμα 'Εξερεύνησης – Εκμετάλλευσης' (Exploration vs Exploitation);",
    "answer": "B",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 241",
      "paragraph": "2η παράγραφος"
    },
    "choices": {
      "A": "Η επιλογή μεταξύ δύο διαφορετικών αλγορίθμων.",
      "B": "Η απόφαση του πράκτορα να δοκιμάσει νέες ενέργειες για απόκτηση γνώσης ή να χρησιμοποιήσει τις ήδη γνωστές για μέγιστο όφελος.",
      "C": "Η σύγκρουση μεταξύ δύο πρακτόρων.",
      "D": "Η επιλογή μεταξύ γρήγορης και αργής εκπαίδευσης."
    },
    "explanation": "Ο πράκτορας πρέπει να ρισκάρει το άγνωστο (exploration) για να βρει ίσως κάτι καλύτερο, αλλά και να 'εξαργυρώσει' όσα έμαθε (exploitation) για να κερδίσει."
  },
  {
    "question": "Πώς η 'Ιεραρχική Ενισχυτική Μάθηση' (HRL) διαχειρίζεται πολύπλοκα προβλήματα;",
    "answer": "C",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 238",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Αγνοεί τις λεπτομέρειες.",
      "B": "Χρησιμοποιεί περισσότερους υπολογιστές.",
      "C": "Διασπά το πρόβλημα σε μικρότερα υπο-προβλήματα (ιεραρχία αποφάσεων), επιτρέποντας πιο αποδοτική μάθηση σε διαφορετικά χρονικά επίπεδα.",
      "D": "Αυξάνει τυχαία τις παραμέτρους."
    },
    "explanation": "Αντί να μαθαίνει μια τεράστια αλυσίδα ενεργειών, το HRL μαθαίνει 'μακρο-εντολές' (π.χ. 'πήγαινε στην πόρτα') που αποτελούνται από απλούστερες κινήσεις."
  },
  {
    "question": "Ποια είναι μια βασική εφαρμογή της Ενισχυτικής Μάθησης στη βιομηχανία;",
    "answer": "D",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 244",
      "paragraph": "1η παράγραφος"
    },
    "choices": {
      "A": "Η συγγραφή εκθέσεων.",
      "B": "Η δημιουργία ιστοσελίδων.",
      "C": "Η αποστολή email.",
      "D": "Η βιομηχανική αυτοματοποίηση και η βελτιστοποίηση γραμμών παραγωγής με χρήση προσαρμοστικών ρομπότ."
    },
    "explanation": "Τα ρομπότ με RL μπορούν να προσαρμόζονται σε αλλαγές στη γραμμή παραγωγής χωρίς επαναπρογραμματισμό, αυξάνοντας την ευελιξία."
  },
  {
    "question": "Τι είναι το 'Curiosity-Driven Learning' (Μάθηση Οδηγούμενη από Περιέργεια);",
    "answer": "A",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 242",
      "paragraph": "3η παράγραφος (υπονοείται στο Reward Sparsity)"
    },
    "choices": {
      "A": "Μια τεχνική όπου ο πράκτορας επιβραβεύεται για την εξερεύνηση άγνωστων καταστάσεων, βοηθώντας όταν οι εξωτερικές ανταμοιβές είναι σπάνιες.",
      "B": "Μάθηση από ερωτήσεις.",
      "C": "Μάθηση μέσω παιχνιδιού.",
      "D": "Μια μέθοδος για εκπαιδευτικούς."
    },
    "explanation": "Όταν ο στόχος είναι μακρινός και δύσκολος, η 'περιέργεια' δίνει κίνητρο στον πράκτορα να συνεχίσει να ψάχνει και να μαθαίνει το περιβάλλον."
  },
  {
    "question": "Ποιο πρόβλημα αντιμετωπίζουν τα συστήματα RL σε περιβάλλοντα 'Sparse Rewards' (Σπανιότητας Ανταμοιβών);",
    "answer": "B",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 242",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Υπερβολική πληροφόρηση.",
      "B": "Ο πράκτορας δυσκολεύεται να μάθει ποια ενέργεια οδήγησε στο αποτέλεσμα, καθώς η επιβράβευση έρχεται πολύ σπάνια ή καθυστερημένα.",
      "C": "Γρήγορη υπερθέρμανση.",
      "D": "Έλλειψη μνήμης."
    },
    "explanation": "Είναι σαν να παίζεις σκάκι και να μαθαίνεις αν έπαιξες καλά μόνο στο τέλος της παρτίδας. Είναι δύσκολο να αποδώσεις την ευθύνη (credit assignment) σε συγκεκριμένες κινήσεις."
  },
  {
    "question": "Πώς χρησιμοποιείται η RL στα 'Έξυπνα Δίκτυα Ενέργειας' (Smart Grids);",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 243",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Για την κατασκευή καλωδίων.",
      "B": "Για την εκτύπωση λογαριασμών.",
      "C": "Για τη δυναμική εξισορρόπηση προσφοράς και ζήτησης, βελτιστοποιώντας την κατανομή ενέργειας σε πραγματικό χρόνο.",
      "D": "Για την αλλαγή λαμπτήρων."
    },
    "explanation": "Οι αλγόριθμοι RL μπορούν να προβλέψουν αιχμές ζήτησης και να ρυθμίσουν αυτόματα τη ροή ενέργειας, ενσωματώνοντας και ανανεώσιμες πηγές."
  },
  {
    "question": "Τι είναι τα 'Model-Based' RL συστήματα;",
    "answer": "D",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 235",
      "paragraph": "4η παράγραφος"
    },
    "choices": {
      "A": "Συστήματα που δεν έχουν μοντέλο.",
      "B": "Συστήματα που μιμούνται ανθρώπους.",
      "C": "Συστήματα που χρησιμοποιούν μόνο εικόνες.",
      "D": "Συστήματα όπου ο πράκτορας κατασκευάζει ένα εσωτερικό μοντέλο του περιβάλλοντος για να προβλέπει τις συνέπειες των ενεργειών του."
    },
    "explanation": "Έχοντας ένα μοντέλο του κόσμου, ο πράκτορας μπορεί να 'φανταστεί' το μέλλον και να σχεδιάσει (planning) πριν δράσει, αντί να μαθαίνει μόνο τυφλά."
  },
  {
    "question": "Ποια είναι η πρόκληση της 'Ασφάλειας' (Safety) στην Ενισχυτική Μάθηση;",
    "answer": "A",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 243",
      "paragraph": "2η παράγραφος"
    },
    "choices": {
      "A": "Η διασφάλιση ότι ο πράκτορας δεν θα προβεί σε επικίνδυνες ενέργειες κατά τη διάρκεια της εξερεύνησης (exploration) στον πραγματικό κόσμο.",
      "B": "Η προστασία από ιούς.",
      "C": "Η ασφάλεια των κωδικών πρόσβασης.",
      "D": "Η φυσική ασφάλεια του υπολογιστή."
    },
    "explanation": "Δεν μπορούμε να αφήσουμε ένα αυτόνομο αυτοκίνητο να 'δοκιμάσει' να τρακάρει για να μάθει ότι είναι κακό. Απαιτούνται ασφαλείς μέθοδοι εξερεύνησης."
  },
  {
    "question": "Τι είναι η 'Off-Policy' μάθηση;",
    "answer": "B",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 237",
      "paragraph": "3η παράγραφος (υπονοείται στο Q-Learning)"
    },
    "choices": {
      "A": "Μάθηση εκτός σύνδεσης.",
      "B": "Μάθηση όπου ο πράκτορας μαθαίνει τη βέλτιστη πολιτική ανεξάρτητα από την πολιτική που ακολουθεί για να συλλέξει δεδομένα.",
      "C": "Μάθηση χωρίς πολιτική.",
      "D": "Μάθηση από λάθη άλλων."
    },
    "explanation": "Επιτρέπει στον πράκτορα να μαθαίνει από παλιά δεδομένα ή από την παρατήρηση άλλων πρακτόρων, χωρίς να χρειάζεται να ακολουθεί ο ίδιος τη βέλτιστη στρατηγική εκείνη τη στιγμή."
  },
  {
    "question": "Στη μελέτη περίπτωσης (σελ. 245), πώς η RL βελτίωσε τη διαχείριση των μικροδικτύων;",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 246",
      "paragraph": "2η παράγραφος"
    },
    "choices": {
      "A": "Αυξάνοντας την τιμή της ενέργειας.",
      "B": "Μειώνοντας την παραγωγή.",
      "C": "Ελαχιστοποιώντας το κόστος λειτουργίας και τις απώλειες ενέργειας μέσω έξυπνης αποθήκευσης και κατανομής.",
      "D": "Καταργώντας τις μπαταρίες."
    },
    "explanation": "Το σύστημα έμαθε πότε να φορτίζει τις μπαταρίες (όταν το ρεύμα είναι φθηνό/υπάρχει ήλιος) και πότε να τις εκφορτίζει, βελτιστοποιώντας την οικονομία."
  },
  {
    "question": "Τι είναι η 'Συνεργατική Μάθηση' (Cooperative Learning) στα MAS;",
    "answer": "D",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 239",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Μάθηση σε ομάδες μαθητών.",
      "B": "Ανταγωνισμός για πόρους.",
      "C": "Μάθηση μέσω αντιγραφής.",
      "D": "Όταν πολλοί πράκτορες εργάζονται μαζί για την επίτευξη ενός κοινού στόχου, μοιραζόμενοι πληροφορίες και ανταμοιβές."
    },
    "explanation": "Σε αντίθεση με τον ανταγωνισμό (zero-sum games), εδώ η επιτυχία της ομάδας μεγιστοποιεί το όφελος για όλους (π.χ. σμήνος drones)."
  },
  {
    "question": "Ποιο είναι το κύριο πλεονέκτημα των μεθόδων 'Policy Gradient';",
    "answer": "A",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 237",
      "paragraph": "4η παράγραφος"
    },
    "choices": {
      "A": "Μπορούν να χειριστούν συνεχείς χώρους ενεργειών (continuous action spaces) και στοχαστικές πολιτικές.",
      "B": "Είναι πιο απλές μαθηματικά.",
      "C": "Δεν χρειάζονται νευρωνικά δίκτυα.",
      "D": "Εγγυώνται πάντα το ολικό βέλτιστο."
    },
    "explanation": "Ενώ το Q-learning δουλεύει καλά με διακριτές επιλογές (αριστερά/δεξιά), το Policy Gradient είναι απαραίτητο για τον ακριβή έλεγχο π.χ. ενός ρομποτικού βραχίονα."
  },
  {
    "question": "Τι είναι το 'Experience Replay' στο DRL;",
    "answer": "B",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 237",
      "paragraph": "2η παράγραφος (υπονοείται στο DQN)"
    },
    "choices": {
      "A": "Η επανάληψη του παιχνιδιού.",
      "B": "Η αποθήκευση προηγούμενων εμπειριών σε μνήμη και η τυχαία ανάκλησή τους για εκπαίδευση, σπάζοντας τη συσχέτιση των δεδομένων.",
      "C": "Η διαγραφή εμπειριών.",
      "D": "Η καταγραφή βίντεο."
    },
    "explanation": "Αυτό σταθεροποιεί την εκπαίδευση των νευρωνικών δικτύων, καθώς τα τυχαία δείγματα μειώνουν την πιθανότητα το δίκτυο να 'κολλήσει' σε πρόσφατα μοτίβα."
  },
  {
    "question": "Πώς η RL συμβάλλει στα 'Αυτόνομα Οχήματα';",
    "answer": "C",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 243",
      "paragraph": "4η παράγραφος"
    },
    "choices": {
      "A": "Τους μαθαίνει να κορνάρουν.",
      "B": "Ελέγχει το ραδιόφωνο.",
      "C": "Επιτρέπει στο όχημα να μάθει πώς να πλοηγείται σε περίπλοκα περιβάλλοντα μέσω συνεχούς προσαρμογής και βελτίωσης της οδηγικής συμπεριφοράς.",
      "D": "Αλλάζει τα λάστιχα."
    },
    "explanation": "Μέσω προσομοιώσεων και πραγματικών δεδομένων, το RL μαθαίνει στο αυτοκίνητο να αντιδρά σωστά σε απρόβλεπτες καταστάσεις στο δρόμο."
  },
  {
    "question": "Τι είναι το 'Sim-to-Real Transfer' (Μεταφορά από Προσομοίωση στην Πραγματικότητα);",
    "answer": "D",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 242",
      "paragraph": "4η παράγραφος"
    },
    "choices": {
      "A": "Μεταφορά αρχείων.",
      "B": "Εικονική πραγματικότητα.",
      "C": "Παιχνίδια προσομοίωσης.",
      "D": "Η πρόκληση της εφαρμογής μιας πολιτικής που εκπαιδεύτηκε σε προσομοιωτή στον φυσικό κόσμο, όπου οι συνθήκες μπορεί να διαφέρουν."
    },
    "explanation": "Οι προσομοιωτές δεν είναι τέλειοι. Το 'Reality Gap' μπορεί να κάνει ένα ρομπότ που πετάει τέλεια στον υπολογιστή να πέφτει στον αληθινό κόσμο."
  }
]
