[
  {
    "question": "Ποιος είναι ο βασικός ρόλος ενός 'Πράκτορα' (Agent) σε ένα σύστημα Ενισχυτικής Μάθησης;",
    "answer": "A",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 233",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Να αλληλεπιδρά με το περιβάλλον και να μαθαίνει μέσω ανταμοιβών ή ποινών",
      "B": "Να αποθηκεύει μόνο δεδομένα",
      "C": "Να εκτελεί προκαθορισμένες εντολές χωρίς μάθηση",
      "D": "Να σχεδιάζει γραφικά περιβάλλοντα"
    },
    "explanation": "Στην Ενισχυτική Μάθηση, ο Πράκτορας είναι η οντότητα που λαμβάνει αποφάσεις, δρα στο περιβάλλον και προσαρμόζει τη συμπεριφορά του βάσει της ανατροφοδότησης."
  },
  {
    "question": "Τι ονομάζουμε 'Περιβάλλον' (Environment) στην Ενισχυτική Μάθηση;",
    "answer": "B",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 233",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Τον φυσικό κόσμο γύρω μας",
      "B": "Το σύστημα με το οποίο αλληλεπιδρά ο πράκτορας και παρέχει ανατροφοδότηση",
      "C": "Το λειτουργικό σύστημα του υπολογιστή",
      "D": "Τον χώρο εργασίας του προγραμματιστή"
    },
    "explanation": "Το Περιβάλλον είναι το πλαίσιο μέσα στο οποίο λειτουργεί ο πράκτορας, αντιδρώντας στις ενέργειές του και επιστρέφοντας νέες καταστάσεις και ανταμοιβές."
  },
  {
    "question": "Ποια είναι η λειτουργία της 'Στρατηγικής' (Policy) σε ένα σύστημα RL;",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 234",
      "paragraph": "Λίστα εννοιών"
    },
    "choices": {
      "A": "Να καθορίζει το hardware του συστήματος",
      "B": "Να υπολογίζει το τελικό σκορ",
      "C": "Να καθορίζει τη συμπεριφορά του πράκτορα επιλέγοντας ενέργειες σε κάθε κατάσταση",
      "D": "Να διαγράφει λανθασμένες κινήσεις"
    },
    "explanation": "Η Στρατηγική (Policy) είναι ο κανόνας ή ο χάρτης που υπαγορεύει στον πράκτορα ποια ενέργεια να επιλέξει δεδομένης μιας συγκεκριμένης κατάστασης."
  },
  {
    "question": "Τι αντιπροσωπεύει η 'Ανταμοιβή' (Reward);",
    "answer": "D",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 233",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Ένα χρηματικό έπαθλο για τον προγραμματιστή",
      "B": "Την ταχύτητα του επεξεργαστή",
      "C": "Τον αριθμό των σφαλμάτων",
      "D": "Το σήμα ανατροφοδότησης που δείχνει την επιτυχία ή αποτυχία μιας ενέργειας"
    },
    "explanation": "Η Ανταμοιβή είναι το κρίσιμο σήμα που λαμβάνει ο πράκτορας από το περιβάλλον για να αξιολογήσει πόσο καλή ή κακή ήταν η ενέργειά του."
  },
  {
    "question": "Τι είναι το 'Deep Reinforcement Learning' (DRL);",
    "answer": "A",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 237",
      "paragraph": "1η παράγραφος (υπό 8.3.1)"
    },
    "choices": {
      "A": "Συνδυασμός Ενισχυτικής Μάθησης με Βαθιά Νευρωνικά Δίκτυα",
      "B": "Μια τεχνική για βαθύ καθαρισμό δεδομένων",
      "C": "Μάθηση σε μεγάλο βάθος θάλασσας",
      "D": "Ενισχυτική μάθηση με πολύπλοκους μαθηματικούς τύπους μόνο"
    },
    "explanation": "Το DRL ενσωματώνει τη δύναμη των βαθιών νευρωνικών δικτύων για την αναπαράσταση καταστάσεων και στρατηγικών σε πολύπλοκα προβλήματα."
  },
  {
    "question": "Ποιο είναι το κύριο χαρακτηριστικό των 'Συστημάτων Πολλαπλών Πρακτόρων' (Multi-Agent Systems);",
    "answer": "B",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 239",
      "paragraph": "1η παράγραφος"
    },
    "choices": {
      "A": "Έχουν έναν πολύ δυνατό πράκτορα",
      "B": "Περιλαμβάνουν πολλούς πράκτορες που αλληλεπιδρούν στο ίδιο περιβάλλον",
      "C": "Δεν έχουν καθόλου πράκτορες",
      "D": "Λειτουργούν μόνο σε θεωρητικό επίπεδο"
    },
    "explanation": "Τα Multi-Agent Systems μελετούν τη συμπεριφορά και την αλληλεπίδραση (συνεργασία ή ανταγωνισμό) πολλών πρακτόρων στον ίδιο χώρο."
  },
  {
    "question": "Ποια πρόκληση περιγράφει το 'Δίλημμα Εξερεύνησης – Εκμετάλλευσης';",
    "answer": "C",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 241",
      "paragraph": "2η παράγραφος"
    },
    "choices": {
      "A": "Το αν πρέπει να αγοράσουμε νέο υπολογιστή",
      "B": "Το αν θα χρησιμοποιήσουμε Wi-Fi ή Ethernet",
      "C": "Την ισορροπία μεταξύ δοκιμής νέων ενεργειών και επιλογής των ήδη γνωστών βέλτιστων",
      "D": "Το πρόβλημα της έλλειψης μνήμης"
    },
    "explanation": "Ο πράκτορας πρέπει να αποφασίσει αν θα ρισκάρει να δοκιμάσει κάτι νέο (εξερεύνηση) για πιθανή μεγαλύτερη ανταμοιβή ή θα ακολουθήσει το σίγουρο μονοπάτι (εκμετάλλευση)."
  },
  {
    "question": "Τι σημαίνει 'Sparsity of Rewards' (Σπανιότητα Ανταμοιβών);",
    "answer": "D",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 242",
      "paragraph": "2η παράγραφος (υπό 8.5.3)"
    },
    "choices": {
      "A": "Ότι οι ανταμοιβές είναι πολύ μικρές σε αξία",
      "B": "Ότι το σύστημα δίνει λάθος ανταμοιβές",
      "C": "Ότι ο πράκτορας δεν θέλει ανταμοιβές",
      "D": "Ότι ο πράκτορας λαμβάνει ανατροφοδότηση σπάνια, μετά από πολλές ενέργειες"
    },
    "explanation": "Σε πολλά προβλήματα, η επιτυχία έρχεται μόνο στο τέλος (π.χ. νίκη σε σκάκι), δυσκολεύοντας τον πράκτορα να καταλάβει ποιες ενδιάμεσες κινήσεις ήταν καλές."
  },
  {
    "question": "Ποια είναι μια πρακτική εφαρμογή της Ενισχυτικής Μάθησης στα ενεργειακά δίκτυα;",
    "answer": "A",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 243",
      "paragraph": "2η παράγραφος (υπό 8.6.1)"
    },
    "choices": {
      "A": "Δυναμική διαχείριση και βελτιστοποίηση διανομής ενέργειας",
      "B": "Κατασκευή καλωδίων",
      "C": "Παραγωγή πετρελαίου",
      "D": "Μέτρηση θερμοκρασίας δωματίου"
    },
    "explanation": "Οι αλγόριθμοι RL μπορούν να ρυθμίζουν έξυπνα τη ροή ενέργειας σε smart grids, εξισορροπώντας προσφορά και ζήτηση σε πραγματικό χρόνο."
  },
  {
    "question": "Τι είναι τα μοντέλα 'Actor-Critic';",
    "answer": "B",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 238",
      "paragraph": "1η παράγραφος (υπό 8.3.3)"
    },
    "choices": {
      "A": "Μοντέλα για κινηματογραφικές ταινίες",
      "B": "Μοντέλα που συνδυάζουν έναν πράκτορα που δρα (Actor) και έναν που αξιολογεί (Critic)",
      "C": "Μοντέλα που κρίνουν μόνο αρνητικά",
      "D": "Μοντέλα χωρίς εκπαίδευση"
    },
    "explanation": "Αυτή η αρχιτεκτονική χρησιμοποιεί δύο δίκτυα: ο Actor προτείνει ενέργειες και ο Critic εκτιμά την αξία τους, βελτιώνοντας τη μάθηση."
  },
  {
    "question": "Ποιος είναι ο στόχος της 'Ιεραρχικής Ενισχυτικής Μάθησης' (Hierarchical RL);",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 238",
      "paragraph": "2η παράγραφος (υπό 8.3.4)"
    },
    "choices": {
      "A": "Να καταργήσει την ιεραρχία",
      "B": "Να κάνει τα πράγματα πιο απλά",
      "C": "Η διάσπαση σύνθετων προβλημάτων σε μικρότερα υπο-προβλήματα",
      "D": "Η αύξηση του μεγέθους των δεδομένων"
    },
    "explanation": "Η ιεραρχική προσέγγιση επιτρέπει την επίλυση πολύπλοκων εργασιών χωρίζοντάς τις σε επιμέρους στόχους και στρατηγικές."
  },
  {
    "question": "Ποια μέθοδος χρησιμοποιείται για την προσαρμογή σε δυναμικά περιβάλλοντα;",
    "answer": "D",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 243",
      "paragraph": "1η παράγραφος (υπό 8.5.4)"
    },
    "choices": {
      "A": "Στατική εκπαίδευση",
      "B": "Αποθήκευση σε CD",
      "C": "Αγνόηση αλλαγών",
      "D": "Μηχανισμοί διαρκούς μάθησης και ταχείας προσαρμογής"
    },
    "explanation": "Σε περιβάλλοντα που αλλάζουν, ο πράκτορας πρέπει να μπορεί να μαθαίνει συνεχώς (continuous learning) και να προσαρμόζεται γρήγορα."
  },
  {
    "question": "Τι ρόλο παίζει η 'Συνάρτηση Τιμής' (Value Function);",
    "answer": "A",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 234",
      "paragraph": "Λίστα εννοιών"
    },
    "choices": {
      "A": "Εκτιμά τη μακροπρόθεσμη αναμενόμενη ανταμοιβή από μια κατάσταση",
      "B": "Υπολογίζει την τιμή πώλησης του προϊόντος",
      "C": "Μετράει το χρόνο εκτέλεσης",
      "D": "Καθορίζει το χρώμα των γραφικών"
    },
    "explanation": "Η Value Function προβλέπει πόσο 'καλή' είναι μια κατάσταση μακροπρόθεσμα, βοηθώντας τον πράκτορα να σχεδιάσει το μέλλον."
  },
  {
    "question": "Ποια εφαρμογή RL αφορά την κίνηση ρομπότ;",
    "answer": "B",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 243",
      "paragraph": "3η παράγραφος (υπό 8.6.2)"
    },
    "choices": {
      "A": "Επεξεργασία κειμένου",
      "B": "Αυτόνομα οχήματα και συστήματα πλοήγησης",
      "C": "Σχεδιασμός ιστοσελίδων",
      "D": "Ανάλυση λογιστικών φύλλων"
    },
    "explanation": "Η RL είναι ιδανική για τον έλεγχο αυτόνομων οχημάτων, επιτρέποντάς τους να μαθαίνουν πώς να πλοηγούνται και να αποφεύγουν εμπόδια."
  },
  {
    "question": "Ποιο είναι το πρόβλημα της 'Υπολογιστικής Πολυπλοκότητας' στο RL;",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 242",
      "paragraph": "1η παράγραφος (υπό 8.5.2)"
    },
    "choices": {
      "A": "Οι υπολογιστές είναι ακριβοί",
      "B": "Τα μαθηματικά είναι δύσκολα",
      "C": "Οι τεράστιες απαιτήσεις σε πόρους και χρόνο για εκπαίδευση σε μεγάλους χώρους καταστάσεων",
      "D": "Η έλλειψη προγραμματιστών"
    },
    "explanation": "Καθώς αυξάνεται η πολυπλοκότητα του προβλήματος, ο αριθμός των καταστάσεων εκτοξεύεται, απαιτώντας τεράστια υπολογιστική ισχύ."
  },
  {
    "question": "Τι συμβαίνει σε ένα 'Ανταγωνιστικό' περιβάλλον πολλαπλών πρακτόρων;",
    "answer": "D",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 239",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Όλοι οι πράκτορες κοιμούνται",
      "B": "Οι πράκτορες ανταλλάσσουν δώρα",
      "C": "Οι πράκτορες συνεργάζονται για κοινό σκοπό",
      "D": "Οι πράκτορες προσπαθούν να μεγιστοποιήσουν το δικό τους κέρδος εις βάρος των άλλων"
    },
    "explanation": "Σε ανταγωνιστικά σενάρια (π.χ. παιχνίδια zero-sum), η επιτυχία ενός πράκτορα σημαίνει συχνά αποτυχία για τους υπόλοιπους."
  },
  {
    "question": "Ποια τεχνική χρησιμοποιείται για τη βελτίωση της πολιτικής μέσω της κλίσης;",
    "answer": "A",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 237",
      "paragraph": "2η παράγραφος (υπό 8.3.2)"
    },
    "choices": {
      "A": "Policy Gradient Methods",
      "B": "Linear Regression",
      "C": "K-Means Clustering",
      "D": "Decision Trees"
    },
    "explanation": "Οι μέθοδοι Policy Gradient βελτιστοποιούν απευθείας τη συνάρτηση πολιτικής, προσαρμόζοντας τα βάρη προς την κατεύθυνση μεγαλύτερης ανταμοιβής."
  },
  {
    "question": "Ποια είναι η κύρια διαφορά της Ενισχυτικής Μάθησης από την Επιβλεπόμενη;",
    "answer": "B",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 233",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Η Ενισχυτική Μάθηση χρησιμοποιεί ετικέτες",
      "B": "Η Ενισχυτική Μάθηση βασίζεται σε αλληλεπίδραση και ανατροφοδότηση, όχι σε έτοιμα παραδείγματα",
      "C": "Η Επιβλεπόμενη Μάθηση δεν έχει δεδομένα",
      "D": "Δεν έχουν διαφορά"
    },
    "explanation": "Στο RL δεν υπάρχουν σωστά ζεύγη εισόδου-εξόδου εκ των προτέρων· ο πράκτορας πρέπει να ανακαλύψει τη λύση μέσω δοκιμής."
  },
  {
    "question": "Ποιο ηθικό ζήτημα ανακύπτει στα συστήματα RL;",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 243",
      "paragraph": "2η παράγραφος (υπό 8.5.5)"
    },
    "choices": {
      "A": "Το κόστος του ρεύματος",
      "B": "Η ταχύτητα του διαδικτύου",
      "C": "Η ευθύνη και η ασφάλεια αποφάσεων που επηρεάζουν ανθρώπους",
      "D": "Η αισθητική των ρομπότ"
    },
    "explanation": "Όταν αλγόριθμοι παίρνουν αυτόνομες αποφάσεις (π.χ. σε οχήματα), τίθενται σοβαρά θέματα ασφάλειας και ηθικής ευθύνης."
  },
  {
    "question": "Τι επιτυγχάνεται με τη 'Συνεργασία' (Cooperation) σε συστήματα πολλαπλών πρακτόρων;",
    "answer": "D",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 239",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Μείωση της απόδοσης",
      "B": "Σύγκρουση συμφερόντων",
      "C": "Καταστροφή του περιβάλλοντος",
      "D": "Επίτευξη κοινού στόχου που δεν μπορεί να επιτευχθεί μεμονωμένα"
    },
    "explanation": "Η συνεργασία επιτρέπει σε ομάδες πρακτόρων να συντονίζουν τις δράσεις τους για να λύσουν προβλήματα που υπερβαίνουν τις ατομικές τους ικανότητες."
  },
  {
    "question": "Ποιο είναι ένα παράδειγμα εφαρμογής RL στις 'Έξυπνες Πόλεις';",
    "answer": "A",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 244",
      "paragraph": "2η παράγραφος (υπό 8.6.4)"
    },
    "choices": {
      "A": "Ευφυή συστήματα ελέγχου κυκλοφορίας (φανάρια)",
      "B": "Κατασκευή κτιρίων",
      "C": "Συλλογή σκουπιδιών με το χέρι",
      "D": "Φύτευση δέντρων"
    },
    "explanation": "Η RL μπορεί να βελτιστοποιήσει τους χρόνους των φαναριών σε πραγματικό χρόνο, μειώνοντας την κίνηση και την κατανάλωση καυσίμων."
  },
  {
    "question": "Τι είναι η 'Πολιτική' (Policy) με απλά λόγια;",
    "answer": "B",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 234",
      "paragraph": "Λίστα εννοιών"
    },
    "choices": {
      "A": "Ένας νόμος του κράτους",
      "B": "Η στρατηγική συμπεριφοράς του πράκτορα",
      "C": "Ένα έγγραφο κειμένου",
      "D": "Μια εικόνα"
    },
    "explanation": "Η πολιτική είναι ουσιαστικά ο 'εγκέφαλος' του πράκτορα, που του λέει τι να κάνει σε κάθε περίσταση."
  },
  {
    "question": "Ποια είναι η συνέπεια της υπερβολικής 'Εκμετάλλευσης' (Exploitation);",
    "answer": "C",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 242",
      "paragraph": "1η παράγραφος (υπό 8.5.1)"
    },
    "choices": {
      "A": "Ο πράκτορας μαθαίνει τα πάντα",
      "B": "Ο πράκτορας γίνεται πλούσιος",
      "C": "Ο πράκτορας μπορεί να εγκλωβιστεί σε υποβέλτιστες λύσεις",
      "D": "Ο πράκτορας σταματάει να λειτουργεί"
    },
    "explanation": "Αν ο πράκτορας επιλέγει μόνο ό,τι ξέρει ήδη ότι δουλεύει, μπορεί να χάσει καλύτερες επιλογές που δεν έχει δοκιμάσει ακόμα."
  },
  {
    "question": "Ποιο συστατικό λαμβάνει ο πράκτορας από το περιβάλλον εκτός από την ανταμοιβή;",
    "answer": "D",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 233",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Χρήματα",
      "B": "Φαγητό",
      "C": "Οδηγίες χρήσης",
      "D": "Την τρέχουσα κατάσταση (State)"
    },
    "explanation": "Ο κύκλος αλληλεπίδρασης περιλαμβάνει: κατάσταση -> ενέργεια -> ανταμοιβή + νέα κατάσταση."
  },
  {
    "question": "Στη μελέτη περίπτωσης για τα Μικροδίκτυα, τι βελτιστοποιεί το σύστημα RL;",
    "answer": "A",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 245",
      "paragraph": "2η παράγραφος (Σκοπός)"
    },
    "choices": {
      "A": "Τη χρήση ανανεώσιμων πηγών και τη μείωση του κόστους ενέργειας",
      "B": "Την ταχύτητα του διαδικτύου",
      "C": "Την ασφάλεια του κτιρίου",
      "D": "Την ποιότητα του νερού"
    },
    "explanation": "Το σύστημα στοχεύει στην έξυπνη διαχείριση μπαταριών και πηγών ενέργειας για οικονομικό και οικολογικό όφελος."
  },
  {
    "question": "Τι κάνει η 'Βιομηχανική Αυτοματοποίηση' με RL;",
    "answer": "B",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 244",
      "paragraph": "1η παράγραφος (υπό 8.6.3)"
    },
    "choices": {
      "A": "Απολύει όλους τους εργάτες",
      "B": "Προσαρμόζει τις γραμμές παραγωγής σε πραγματικό χρόνο για βέλτιστη ροή",
      "C": "Σταματάει την παραγωγή",
      "D": "Βάφει τα προϊόντα"
    },
    "explanation": "Τα προσαρμοστικά συστήματα παραγωγής χρησιμοποιούν RL για να ρυθμίζουν ταχύτητες και να αντιμετωπίζουν προβλήματα χωρίς ανθρώπινη παρέμβαση."
  },
  {
    "question": "Ποιο είναι το χαρακτηριστικό των 'Μοντέλων χωρίς Μοντέλο' (Model-Free);",
    "answer": "C",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 237",
      "paragraph": "1η παράγραφος (έμμεση αναφορά σε μεθόδους)"
    },
    "choices": {
      "A": "Έχουν τέλειο χάρτη του περιβάλλοντος",
      "B": "Δεν μαθαίνουν τίποτα",
      "C": "Μαθαίνουν απευθείας από την εμπειρία χωρίς να γνωρίζουν τους κανόνες του περιβάλλοντος",
      "D": "Είναι πολύ αργά"
    },
    "explanation": "Οι Model-Free αλγόριθμοι (όπως το Q-Learning) δεν προσπαθούν να μοντελοποιήσουν πώς λειτουργεί ο κόσμος, αλλά μαθαίνουν τι να κάνουν μέσω πειραματισμού."
  },
  {
    "question": "Ποια είναι η σχέση των ενεργειών και των ανταμοιβών στο RL;",
    "answer": "D",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 233",
      "paragraph": "3η παράγραφος"
    },
    "choices": {
      "A": "Είναι τυχαία",
      "B": "Δεν έχουν σχέση",
      "C": "Οι ανταμοιβές δίνονται πριν τις ενέργειες",
      "D": "Οι ενέργειες οδηγούν σε ανταμοιβές ή ποινές, καθοδηγώντας τη μάθηση"
    },
    "explanation": "Η αιτιώδης σχέση 'Ενέργεια -> Αποτέλεσμα (Ανταμοιβή)' είναι ο πυρήνας της εκπαίδευσης του πράκτορα."
  },
  {
    "question": "Ποιο πρόβλημα λύνει η 'Εξερεύνηση' (Exploration);",
    "answer": "A",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 241",
      "paragraph": "2η παράγραφος (υπό 8.5.1)"
    },
    "choices": {
      "A": "Την ανακάλυψη νέων, πιθανώς καλύτερων στρατηγικών",
      "B": "Την εξοικονόμηση ενέργειας",
      "C": "Τη μείωση του θορύβου",
      "D": "Την ταξινόμηση δεδομένων"
    },
    "explanation": "Χωρίς εξερεύνηση, ο πράκτορας θα έμενε στάσιμος στις πρώτες λύσεις που βρήκε, αγνοώντας πιθανές βελτιώσεις."
  },
  {
    "question": "Τι είναι το 'Actor' στο μοντέλο Actor-Critic;",
    "answer": "B",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 238",
      "paragraph": "1η παράγραφος (υπό 8.3.3)"
    },
    "choices": {
      "A": "Αξιολογεί την κατάσταση",
      "B": "Επιλέγει την ενέργεια προς εκτέλεση",
      "C": "Παρακολουθεί το περιβάλλον",
      "D": "Δίνει την ανταμοιβή"
    },
    "explanation": "Ο Actor είναι το κομμάτι του δικτύου που παίρνει την απόφαση για το ποια δράση θα εκτελεστεί."
  },
  {
    "question": "Τι είναι το 'Critic' στο μοντέλο Actor-Critic;",
    "answer": "C",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 238",
      "paragraph": "1η παράγραφος (υπό 8.3.3)"
    },
    "choices": {
      "A": "Επιλέγει την ενέργεια",
      "B": "Εκτελεί την ενέργεια",
      "C": "Αξιολογεί την ποιότητα της επιλεγμένης ενέργειας",
      "D": "Διαγράφει το μοντέλο"
    },
    "explanation": "Ο Critic κρίνει την απόφαση του Actor, παρέχοντας ανατροφοδότηση για το αν η ενέργεια ήταν καλή ή κακή."
  },
  {
    "question": "Ποια είναι η πρόκληση της 'Αβεβαιότητας' στο περιβάλλον;",
    "answer": "D",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 242",
      "paragraph": "2η παράγραφος (υπό 8.5.3)"
    },
    "choices": {
      "A": "Δεν υπάρχουν κανόνες",
      "B": "Το περιβάλλον είναι σκοτεινό",
      "C": "Ο πράκτορας δεν βλέπει",
      "D": "Η ίδια ενέργεια μπορεί να έχει διαφορετικά αποτελέσματα ανάλογα με τυχαίους παράγοντες"
    },
    "explanation": "Στοχαστικά περιβάλλοντα εισάγουν θόρυβο, κάνοντας δύσκολη την πρόβλεψη του ακριβούς αποτελέσματος μιας ενέργειας."
  },
  {
    "question": "Πώς βοηθά το RL στη 'Δυναμική Τιμολόγηση' (Dynamic Pricing);",
    "answer": "A",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 245",
      "paragraph": "2η παράγραφος (υπό 8.7)"
    },
    "choices": {
      "A": "Προσαρμόζει τις τιμές βάσει ζήτησης και προσφοράς σε πραγματικό χρόνο",
      "B": "Κρατάει τις τιμές σταθερές",
      "C": "Δίνει όλα τα προϊόντα δωρεάν",
      "D": "Αυξάνει τις τιμές τυχαία"
    },
    "explanation": "Στα μικροδίκτυα (και όχι μόνο), η τιμολόγηση προσαρμόζεται δυναμικά για να ενθαρρύνει ή να αποθαρρύνει την κατανάλωση."
  },
  {
    "question": "Τι είδους μάθηση χρησιμοποιούν τα 'Αυτόνομα Drones' για πλοήγηση;",
    "answer": "B",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 243",
      "paragraph": "3η παράγραφος (υπό 8.6.2)"
    },
    "choices": {
      "A": "Supervised Learning",
      "B": "Reinforcement Learning",
      "C": "Unsupervised Learning",
      "D": "Linear Regression"
    },
    "explanation": "Η πλοήγηση σε άγνωστο χώρο απαιτεί συνεχή λήψη αποφάσεων και προσαρμογή, πεδίο στο οποίο η RL υπερέχει."
  },
  {
    "question": "Τι είναι η 'Q-Learning';",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 237",
      "paragraph": "1η παράγραφος (αναφορά σε Q-Learning)"
    },
    "choices": {
      "A": "Μια γλώσσα προγραμματισμού",
      "B": "Ένα παιχνίδι",
      "C": "Ένας θεμελιώδης αλγόριθμος ενισχυτικής μάθησης για την εκτίμηση της αξίας ενεργειών",
      "D": "Μια μάρκα ρομπότ"
    },
    "explanation": "Ο Q-Learning είναι από τους πιο διάσημους αλγορίθμους που μαθαίνει την ποιότητα (Quality) κάθε ενέργειας σε κάθε κατάσταση."
  },
  {
    "question": "Ποιο είναι το βασικό πλεονέκτημα της 'Ιεραρχικής' δομής;",
    "answer": "D",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 238",
      "paragraph": "2η παράγραφος (υπό 8.3.4)"
    },
    "choices": {
      "A": "Είναι πιο φθηνή",
      "B": "Είναι πιο γρήγορη",
      "C": "Είναι πιο όμορφη",
      "D": "Επιτρέπει την επαναχρησιμοποίηση δεξιοτήτων και τη διαχείριση πολυπλοκότητας"
    },
    "explanation": "Μαθαίνοντας επιμέρους δεξιότητες (π.χ. 'περπάτημα'), ο πράκτορας μπορεί να τις συνδυάσει για να λύσει μεγαλύτερα προβλήματα (π.χ. 'φτάσε στο τέρμα')."
  },
  {
    "question": "Τι ορίζει ο 'Ορίζοντας' (Horizon) σε ένα πρόβλημα RL;",
    "answer": "A",
    "difficulty": "hard",
    "reference": {
      "page": "σελ. 234",
      "paragraph": "Λίστα εννοιών (υπονοείται στην Value Function)"
    },
    "choices": {
      "A": "Το πόσο μακριά στο μέλλον κοιτάζει ο πράκτορας για να υπολογίσει την ανταμοιβή",
      "B": "Την απόσταση που μπορεί να δει",
      "C": "Την ώρα που δύει ο ήλιος",
      "D": "Το τέλος του παιχνιδιού"
    },
    "explanation": "Ο ορίζοντας καθορίζει αν ο πράκτορας ενδιαφέρεται για το άμεσο κέρδος ή για τη μακροπρόθεσμη επιτυχία (discount factor)."
  },
  {
    "question": "Ποια είναι η επίδραση των 'θορυβωδών' ανταμοιβών;",
    "answer": "B",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 242",
      "paragraph": "2η παράγραφος (υπό 8.5.3)"
    },
    "choices": {
      "A": "Βελτιώνουν τη μάθηση",
      "B": "Δυσκολεύουν την εκτίμηση της πραγματικής αξίας μιας ενέργειας",
      "C": "Αυξάνουν την ταχύτητα",
      "D": "Δεν έχουν καμία επίδραση"
    },
    "explanation": "Ο θόρυβος στις ανταμοιβές μπορεί να μπερδέψει τον πράκτορα, κάνοντάς τον να νομίζει ότι μια κακή κίνηση ήταν καλή (ή το αντίθετο)."
  },
  {
    "question": "Τι περιλαμβάνει το στάδιο 'Σχεδιασμού' (Planning) στη Μελέτη Περίπτωσης;",
    "answer": "C",
    "difficulty": "medium",
    "reference": {
      "page": "σελ. 246",
      "paragraph": "3η παράγραφος (Φάση 1: Ανάλυση και Σχεδιασμός)"
    },
    "choices": {
      "A": "Την κατασκευή του κτιρίου",
      "B": "Την αγορά εξοπλισμού",
      "C": "Τον καθορισμό των στόχων, των μεταβλητών κατάστασης και των ενεργειών",
      "D": "Την πρόσληψη προσωπικού"
    },
    "explanation": "Πριν την υλοποίηση, πρέπει να οριστούν σαφώς τι θα παρατηρεί ο πράκτορας, τι μπορεί να ελέγξει και ποιος είναι ο στόχος."
  },
  {
    "question": "Ποιο αποτέλεσμα είχε η εφαρμογή του συστήματος RL στη μελέτη περίπτωσης;",
    "answer": "D",
    "difficulty": "easy",
    "reference": {
      "page": "σελ. 247",
      "paragraph": "2η παράγραφος (Αποτελέσματα)"
    },
    "choices": {
      "A": "Αύξηση του κόστους",
      "B": "Καμία αλλαγή",
      "C": "Διακοπές ρεύματος",
      "D": "Μείωση του κόστους ενέργειας κατά 15% και βελτίωση της σταθερότητας"
    },
    "explanation": "Η έξυπνη διαχείριση οδήγησε σε μετρήσιμη οικονομική βελτίωση και καλύτερη αξιοποίηση των ανανεώσιμων πηγών."
  }
]
